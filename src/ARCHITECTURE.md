# A1Facts Benchmark - Complete Structure

## ğŸ“ Directory Tree

```
c:\a1facts\benchmark\
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          Main documentation
â”œâ”€â”€ ğŸ“„ SETUP_GUIDE.md                     Setup instructions
â”œâ”€â”€ ğŸ“„ PROJECT_SUMMARY.md                 Project overview
â”œâ”€â”€ ğŸ“„ requirements.txt                   Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example                       Environment template
â”œâ”€â”€ ğŸ“„ .gitignore                         Git ignore rules
â”œâ”€â”€ ğŸ“„ run_benchmark.py                   ğŸš€ Main execution script
â”‚
â”œâ”€â”€ ğŸ“‚ src/                               Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ data_generation/              Test case generation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ domain_authority.py          80+ domain reliability mappings
â”‚   â”‚   â””â”€â”€ generate_dataset.py          Test case generator (14 cases)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/                       LLM wrappers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py                Abstract interface
â”‚   â”‚   â”œâ”€â”€ gpt4_model.py                OpenAI GPT-4o
â”‚   â”‚   â”œâ”€â”€ claude_model.py              Anthropic Claude 3.5
â”‚   â”‚   â””â”€â”€ gemini_model.py              Google Gemini 2.0
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ evaluation/                   Evaluation pipeline
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ run_benchmark.py             Main evaluation + Comet
â”‚       â””â”€â”€ analyze_results.py           Analysis & visualization
â”‚
â”œâ”€â”€ ğŸ“‚ datasets/                         Test datasets
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ (generated) triangulation_benchmark_v1.json
â”‚
â”œâ”€â”€ ğŸ“‚ results/                          Evaluation outputs
â”‚   â””â”€â”€ (generated) evaluation_results_*.json
â”‚   â””â”€â”€ (generated) *.png, *.csv
â”‚
â”œâ”€â”€ ğŸ“‚ experiments/                      Comet experiment logs
â”‚
â””â”€â”€ ğŸ“‚ notebooks/                        Jupyter analysis notebooks
```

## ğŸ¯ What Each File Does

### Core Execution
| File | Purpose | Usage |
|------|---------|-------|
| `run_benchmark.py` | One-command execution | `python run_benchmark.py` |
| `requirements.txt` | Dependencies | `pip install -r requirements.txt` |
| `.env.example` | API key template | Copy to `.env` and configure |

### Data Generation
| File | Purpose | Key Features |
|------|---------|--------------|
| `domain_authority.py` | Website reliability mappings | 80+ domains (NIH=A, Mayo=B, WebMD=C, etc.) |
| `generate_dataset.py` | Test case generator | 14 objective cases across 6 validity ratings |

### Model Integration
| File | Model | API |
|------|-------|-----|
| `gpt4_model.py` | GPT-4o | OpenAI |
| `claude_model.py` | Claude 3.5 Sonnet | Anthropic |
| `gemini_model.py` | Gemini 2.0 Flash | Google |
| `base_model.py` | Interface | All models inherit |

### Evaluation
| File | Purpose | Output |
|------|---------|--------|
| `run_benchmark.py` | Run evaluation | JSON results + Comet logs |
| `analyze_results.py` | Generate reports | CSV tables + PNG charts |

## ğŸ”„ Data Flow

```
1. Dataset Generation
   generate_dataset.py â†’ triangulation_benchmark_v1.json
   
2. Model Evaluation
   run_benchmark.py â†’ Loads dataset
                   â†’ Initializes models (GPT-4o, Claude, Gemini)
                   â†’ Runs evaluation
                   â†’ Logs to Comet
                   â†’ Saves results/*.json
   
3. Analysis
   analyze_results.py â†’ Loads results/*.json
                      â†’ Generates comparison tables
                      â†’ Creates visualizations
                      â†’ Outputs CSV + PNG files
```

## ğŸ“Š Test Cases Breakdown

### Validity Rating 1: Confirmed (3 cases)
- **medical_fact**: CDC + NIH + WHO agree on diabetes prevalence
- **financial_fact**: SEC filing + Reuters + Bloomberg confirm Apple revenue
- **historical_event**: AP + BBC + Reuters confirm 2020 election results

### Validity Rating 2: Probably True (2 cases)
- **market_reaction**: Earnings â†’ stock rise â†’ analyst upgrade (logical chain)
- **medical_correlation**: Drug effectiveness â†’ recommendations â†’ sales (consistent)

### Validity Rating 3: Possibly True (2 cases)
- **geopolitical_implication**: Russia invasion â†’ China readiness â†’ defense stocks
- **tech_speculation**: Apple AI chip â†’ semiconductor stocks â†’ industry expectations

### Validity Rating 4: Doubtful (2 cases)
- **contradiction**: Record revenue contradicts bankruptcy filing
- **logical_inconsistency**: Obesity â†“ contradicts diabetes â†‘ + fast food â†‘

### Validity Rating 5: Improbable (3 cases)
- **mathematical_impossibility**: 5 employees Ã— $50k â‰  $10B revenue
- **temporal_impossibility**: Cannot discontinue before release
- **physical_impossibility**: 500mph Ã— 2hr â‰  3,500 miles

### Validity Rating 6: Cannot Judge (2 cases)
- **unrelated_information**: Weather + Bitcoin + NFL (no connection)
- **insufficient_context**: Vague claims from unknown sources

## ğŸš€ Usage Workflow

### Quick Start (1 command)
```bash
python run_benchmark.py
```

### Manual Steps
```bash
# 1. Generate dataset
cd src/data_generation
python generate_dataset.py

# 2. Run evaluation
cd ../..
python -m src.evaluation.run_benchmark

# 3. Analyze results
python -m src.evaluation.analyze_results results/evaluation_results_*.json
```

### Custom Evaluation
```python
from src.evaluation import BenchmarkEvaluator

evaluator = BenchmarkEvaluator("datasets/triangulation_benchmark_v1.json")

# Evaluate specific models
results = evaluator.run_evaluation(
    model_names=["gpt-4o", "claude-3.5-sonnet"],
    use_comet=True
)

evaluator.save_results(results)
```

## ğŸ“ˆ Expected Metrics

### Overall Accuracy
- Target: >85% for production-ready models
- Measures: Correct validity rating predictions

### Per-Validity Accuracy
- Rating 1 (Confirmed): Should be 100% (obvious agreement)
- Rating 5 (Improbable): Should be >90% (clear impossibilities)
- Rating 3 (Possibly True): Hardest category (subjective boundary)

### Confusion Matrix
- Shows which ratings models confuse (e.g., 2 vs 3)
- Identifies systematic biases

## ğŸ“ Comet Dashboard Metrics

Automatically logged for each experiment:
- `overall_accuracy`: Primary metric
- `validity_1_accuracy` through `validity_6_accuracy`
- `medical_fact_accuracy`, `financial_fact_accuracy`, etc.
- `total_cases`, `correct_predictions`

## âœ… Validation Checklist

Before running:
- [ ] API keys configured in `.env`
- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] Comet account created (free at comet.com)

After running:
- [ ] Check `results/` for JSON output
- [ ] Check Comet dashboard for experiments
- [ ] Review confusion matrices for systematic errors
- [ ] Compare models in CSV table

## ğŸ”§ Troubleshooting

**Import Error: comet_ml**
```bash
pip install comet-ml
```

**API Key Not Found**
```bash
# Edit .env file
OPENAI_API_KEY=sk-...
COMET_API_KEY=...
```

**No Models Available**
- Ensure at least one model API key is set
- Models initialize independently (partial success OK)

## ğŸ“ Next Development Steps

1. **Expand Dataset**: 20+ cases per validity rating (120 total)
2. **Add Domains**: Legal, scientific, technical scenarios
3. **Expert Validation**: Get domain experts to review edge cases
4. **Real-time Events**: Test on post-training-cutoff news
5. **Custom Analysis**: Domain-specific performance metrics

## ğŸ¯ Success Criteria

âœ… Framework evaluates triangulation logic
âœ… Multi-model comparison enabled
âœ… Objective ground truth established
âœ… Comet integration working
âœ… Production-ready code quality
âœ… Extensible architecture
