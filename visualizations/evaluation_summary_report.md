
# A1Facts Benchmark Evaluation Report

## Dataset Overview
- **Total Test Cases**: 134
- **Models Evaluated**: 7
- **Evaluation Date**: October 15, 2025

## ğŸ† Top Performing Model
**gemini-2.5-pro** with overall score of **0.442**

## ğŸ“ˆ Aggregate Performance Metrics

### Tier 1: Strict Accuracy (Both Correct)
- **Average**: 17.2%
- **Best**: 23.9%
- **Worst**: 5.2%

### Tier 2: Individual Accuracies
- **Validity Average**: 44.5%
- **Reliability Average**: 66.2%

### Tier 3: Detailed Analysis
Available in individual visualizations and comprehensive dashboard.

## ğŸ“Š Generated Visualizations
1. `tier1_strict_accuracy.png` - Strict accuracy bar charts
2. `tier2_individual_accuracies.png` - Individual accuracy comparisons
3. `tier3_validity_f1_by_class.png` - Validity F1 scores by class
4. `tier3_reliability_f1_by_grade.png` - Reliability F1 scores by grade
5. `tier3_macro_f1_comparison.png` - Macro F1 score comparisons
6. `model_rankings.png` - Overall model performance rankings
7. `comprehensive_dashboard.html` - Interactive dashboard
8. `model_rankings.csv` - Detailed rankings data

## ğŸ¯ Key Insights
- Strict accuracy remains challenging across all models
- Reliability assessment generally outperforms validity assessment
- Significant variation in per-class performance across validity levels

Generated on: 2025-10-15 22:07:07
